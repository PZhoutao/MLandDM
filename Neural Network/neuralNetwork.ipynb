{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a 3-layer neural network\n",
    "The basic structure of the neural network model is:\n",
    "\n",
    "input x -> z1 = ReLU(W1\\*x+b1) -> z2 = W2\\*z1+b2 -> softmax(z2)\n",
    "\n",
    "The MNIST data from package chainer is used to train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def init_W(shape):\n",
    "'''\n",
    "Use small random numbers to initializ matrix W.\n",
    "Numbers greater than 1 or smaller than -1 are truncated.\n",
    "'''\n",
    "    W = np.random.normal(0,0.5,shape)\n",
    "    W[W > 1] = 1\n",
    "    W[W < -1] = -1\n",
    "    return W\n",
    "\n",
    "def init_b(length):\n",
    "'''\n",
    "Use small random numbers to initializ vector b.\n",
    "Numbers greater than 1 or smaller than -1 are truncated.\n",
    "'''\n",
    "    b = np.random.normal(0,0.5,length)\n",
    "    b[b > 1] = 1\n",
    "    b[b < -1] = -1\n",
    "    return b\n",
    "\n",
    "def softmax(arr):\n",
    "'''\n",
    "Calculate softmax for a vector of numbers.\n",
    "'''\n",
    "    arr = np.exp(arr)\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def foreward(x):\n",
    "'''\n",
    "Do the forward calculation with input vector x.\n",
    "Output softmax result.\n",
    "'''\n",
    "    z1 = np.dot(W1, x) + b1\n",
    "    z1[z1 < 0] = 0\n",
    "    z2 = np.dot(W2, z1) + b2\n",
    "    softm = softmax(z2)\n",
    "    return softm, z1\n",
    "\n",
    "def PD_softm_z2(softm):\n",
    "'''\n",
    "Calculate partial derivative matrix of log(softmax(z2)) to z2.\n",
    "'''\n",
    "    a = np.identity(k)\n",
    "    return (a - np.row_stack([softm]*k))\n",
    "\n",
    "def get_gradient(x,y):\n",
    "'''\n",
    "Calculate gradient for W1, b1, W2, b2\n",
    "'''\n",
    "    grad_W1 = np.zeros((L2, L1))\n",
    "    grad_b1 = np.zeros(L2)\n",
    "    grad_W2 = np.zeros((k, L2))\n",
    "    grad_b2 = np.zeros(k)\n",
    "    for i in range(batch_size):\n",
    "        softm, z1 = foreward(x[i])\n",
    "        J_softm_z2 = PD_softm_z2(softm)\n",
    "        v2 = np.dot(-y[i],J_softm_z2)\n",
    "        grad_W2 = grad_W2 + ((np.row_stack([z1]*k).T)*v2).T\n",
    "        grad_b2 = grad_b2 + v2\n",
    "        v1 = np.dot(v2, W2)\n",
    "        foo = np.row_stack([x[i]]*L2)\n",
    "        is_positive = (z1 > 0).astype(float)\n",
    "        foo = (foo.T*is_positive).T\n",
    "        grad_W1 = grad_W1 + (foo.T*v1).T\n",
    "        grad_b1 = grad_b1 + v1*is_positive\n",
    "    grad_W2 = grad_W2 / batch_size + penalty*W2\n",
    "    grad_b2 = grad_b2 / batch_size + penalty*b2\n",
    "    grad_W1 = grad_W1 / batch_size + penalty*W1\n",
    "    grad_b1 = grad_b1 / batch_size + penalty*b1\n",
    "    return grad_W2, grad_b2, grad_W1, grad_b1\n",
    "\n",
    "def evaluate():\n",
    "'''\n",
    "Make predictions for test data, check test accuracy.\n",
    "'''\n",
    "    y_prediction = []\n",
    "    for i in range(len(test_label)):\n",
    "        softm, z1 = foreward(test_arr[i])\n",
    "        y_prediction.append(np.argmax(softm))\n",
    "    accurate = [a == b for a,b in zip(test_label,y_prediction)]\n",
    "    accuracy = sum(accurate) / len(accurate)\n",
    "    return accuracy\n",
    "\n",
    "def one_hot(a, k):\n",
    "'''\n",
    "Return a vector of length k, with ath element 1 and other 0s\n",
    "'''\n",
    "    arr = np.zeros(k)\n",
    "    arr[a] = 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set global parameters\n",
    "L1 = 784   # length of input vector\n",
    "L2 = 100   # number of units in hidden layer\n",
    "k = 10     # number of class to predict\n",
    "batch_size = 10  # batch size\n",
    "n_batch = 60000 // batch_size  # number of batches for an epoch\n",
    "penalty = 0.0001   # penalty parameter, lambda\n",
    "eta = 0.08  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in data from chainer\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "train_arr = []\n",
    "train_label = []\n",
    "test_arr = []\n",
    "test_label = []\n",
    "for i in range(len(train)):\n",
    "    train_arr.append(train[i][0])\n",
    "    train_label.append(one_hot(train[i][1],k))\n",
    "for i in range(len(test)):\n",
    "    test_arr.append(test[i][0])\n",
    "    test_label.append(test[i][1])\n",
    "train_arr = np.array(train_arr)\n",
    "train_label = np.array(train_label)\n",
    "test_arr = np.array(test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize W1, b1, W2, b2\n",
    "W1 = init_W((L2, L1))\n",
    "b1 = init_b(L2)\n",
    "W2 = init_W((k, L2))\n",
    "b2 = init_b(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 accuracy =  0.9308\n",
      "epoch 2 accuracy =  0.9499\n",
      "epoch 3 accuracy =  0.9567\n",
      "epoch 4 accuracy =  0.9589\n",
      "epoch 5 accuracy =  0.9623\n",
      "epoch 6 accuracy =  0.9648\n",
      "epoch 7 accuracy =  0.967\n",
      "epoch 8 accuracy =  0.969\n",
      "epoch 9 accuracy =  0.97\n",
      "epoch 10 accuracy =  0.9695\n",
      "epoch 11 accuracy =  0.9711\n",
      "epoch 12 accuracy =  0.9718\n",
      "epoch 13 accuracy =  0.9722\n",
      "epoch 14 accuracy =  0.9733\n",
      "epoch 15 accuracy =  0.9727\n",
      "epoch 16 accuracy =  0.9751\n",
      "epoch 17 accuracy =  0.9736\n",
      "epoch 18 accuracy =  0.9742\n",
      "epoch 19 accuracy =  0.9753\n",
      "epoch 20 accuracy =  0.9758\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "accuracy = []\n",
    "for epoch in range(20):\n",
    "    shuffle = list(range(60000))\n",
    "    np.random.shuffle(shuffle)\n",
    "    for i in range(n_batch):\n",
    "        x = train_arr[shuffle[i*batch_size:(i+1)*batch_size]]\n",
    "        y = train_label[shuffle[i*batch_size:(i+1)*batch_size]]\n",
    "        grad_W2, grad_b2, grad_W1, grad_b1 = get_gradient(x,y)\n",
    "        W2 = W2 - eta*grad_W2\n",
    "        b2 = b2 - eta*grad_b2\n",
    "        W1 = W1 - eta*grad_W1\n",
    "        b1 = b1 - eta*grad_b1\n",
    "    acc_epoch = evaluate()\n",
    "    accuracy.append(acc_epoch)\n",
    "    print('epoch '+str(epoch+1)+' accuracy = ', acc_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
